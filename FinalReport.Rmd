---
title: "What is the Value of a University's Pedigree?"
author: "Angela, Jeremy, George, Matt"
date: "May 3, 2018"
output: 
  word_document:
     toc: true
     toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Load Package and Datasets, message=FALSE}
library(perturb)
library(tidyverse)
library(tree)
library(class)
library(car)
library(FNN)
library(lmtest)
library(randomForest)
library(car)
fullDataTraining <- read.csv("FullDataTrainingComplete.csv", header = TRUE, stringsAsFactors = FALSE)
```


The final project report will contain the following sections:

##Section 1 Analytics Question/Problem (5 pts.) 

The analytics question/problem being addressed. In your framing of the analytics question, please state clearly articulate: 

a.	The specific predictive analytic question you are attempting to answer, or problem you are trying to solve? Since this course is about predictive analytics, it is important that you clearly specify the response variable you are predicting. 

b.	What type of problem or question you are addressing – i.e., quantitative or classification 

c.	What your analytics goal(s) is(are) – i.e., inference, interpretation and/or prediction. 


One of the main reasons students attend universities in the US is the hope of getting a better paying job once they graduated. Our hope is to figure out do certain universities provide a better opportunity of doing this than others. Our team wants to predict given a school and several school statistics, can we predict the starting median salary that a graduate from that particular university would earn.

Since our team wants to predict a starting salary this problem is a quantitative problem. Our main goal is to compare salaries that we do not know with salaries that we do know therefore the main analytical goal here is high predictive accuracy.


##Section 2 Business Rational (10 pts.)

A brief but compelling business rationale about the importance of this question/problem from a business perspective. Why is the problem you are analyzing important? 

This analytics question is important from two perspectives: a student and a school.

For the student:

For students entering a university a career is several years away, knowing what the salary is for alumni may influence their decision as where to go. On top of this with the known cost of the university and their predicted salary they would be able to determine a general return on investment as well how long their payback period would be.


For the school:

Even though admission rates throughout the country are decreasing, schools are still competing to attract top talent. While there are numerous reasons to attend schools (close to home, prestige of a university, or top program for a degree) one of the major reasons to attend is money. Going to a university is an investment and the cost of that investment is increasing. Therefore universities need to justify that investment to their customers, one of the best ways to do this is with their alumni starting salary. 


##Section 3 Description of the Data (10 pts.) 

A description of the data set utilized for the analysis (if the data set is not available in an R package or public web site, the data set must be attached). Your data description should be sufficient for your reading audience to understand your data set, variables and the interpretations you provide in your report, including variable types and units of measurement. The data description should be accompanied by any necessary descriptive analytics artifacts necessary for your predictive modeling (e.g., descriptive statistics, correlation matrix, correlation plots, other plots, etc.). 

The data for this project originated from two sources. The first is from a Kaggle Dataset called Where it Pays to Attend College. This data set includes the university, region, and the schools alumnus median starting salary (in \$) and mid-career salary at certain percentiles (in \$). The second was collected from publicly available information on www.collegedata.com, this data includes admission rates (as decimal), number of undergraduates, cost of tuition (in \$), and location of the school. These datasets were combined together and is attached to this report.

Need descriptive statistics correlation matrix and correlation plots other plots



##Section 4 Pre-Processing	 (10 pts.) 

The data was pulled from two source locations: kaggle.com and college.com.  Each of these sites provided the university name, number of undergraduates, admit rates, entrance difficulty, and school location.  After both lists were created, they were merged in order to create a single data set.  The intent of using two sources for the data was to ensure that we had a more complete picture of what the school reported as its admit rates, post undergraduate salaries, and cost of tuition.  Out of the 1800 schools that were pulled from these two sources, 311 of the schools had complete data reported on all the categories that we were identifying as possible factors in post undergraduate salaries. The rest of the schools, we identified the missing variables and replaced blanks with “N/A” in order to allow R to read the data without any blanks. 

To further refine our data set, we utilized a Q-Q Plot to visually see how the data was distributed. The plot identified that the data has some non-normality in it with the starting median salary being skewed right meaning that there may be more extreme values in our data that we did not expect. To correct this, we logged the response variable in order to see if this would normalize the data; however, it did not. The logged Q-Q Plot continued to show a more than non-normal distribution to the data. We then utilized a histogram to further explore how the data was distributed and confirmed that the data was significantly skewed to the right. We then utilized a boxplot in order to identify if there were any data points that “fell” out of the normal distribution and identified that several data points did. This led us to do a function that would identify which data points were significantly different than the rest. There were 17 universities reported starting median salary that is significantly higher than the other universities. This group of schools were identified as “Ivy League” or a technical school that had a reputation of “high caliber” production of students. Since these schools were significantly higher than 75% of the entire data set, we removed these points in order to obtain a more normally distributed data set so that we may further analyze the data. After removing these points, we ran another a normal and a logged Q-Q Plot. Both plots showed a more normal distribution of data but neither plot was more significant than the other.  This is also evident in the histogram that we ran to confirm the data was more normally distributed. We chose to use the non-logged data set to continue with the analysis.

```{r}
qqnorm(fullDataTraining$Starting.Median.Salary, main = "Starting Median Salary Q-Q Plot")
qqline(fullDataTraining$Starting.Median.Salary)

hist(fullDataTraining$Starting.Median.Salary, main = "Histogram of Starting Median Salary", xlab = "Starting Median Salary")

qqnorm(log(fullDataTraining$Starting.Median.Salary), main = "log(Starting Median Salary) Q-Q Plot")
qqline(log(fullDataTraining$Starting.Median.Salary))

hist(log(fullDataTraining$Starting.Median.Salary), main = "Histogram of log(Starting Median Salary)", xlab = "log(Starting Median Salary)")

boxplot(fullDataTraining$Starting.Median.Salary, main = "Boxplot of Starting Median Salary")

calc.limits <- function(x, na.rm = TRUE) {
    qnt <- quantile(x, probs=c(.25, .75), na.rm = na.rm)
    H <- 1.5 * IQR(x, na.rm = na.rm)
    lwr <- qnt[1] - H
    upr <- qnt[2] + H
    c(lwr, upr)
}
calc.limits(fullDataTraining$Starting.Median.Salary)
fDTNoOut<- fullDataTraining%>% 
  filter(Starting.Median.Salary>32475 & Starting.Median.Salary<58275)

#University Outliers are as follows:
setdiff(fullDataTraining,fDTNoOut) %>% arrange(desc(Starting.Median.Salary)) %>% select(University, Starting.Median.Salary)

qqnorm(fDTNoOut$Starting.Median.Salary, main = "Starting Median Salary (Without Outliers) Q-Q Plot")
qqline(fDTNoOut$Starting.Median.Salary)

hist(fDTNoOut$Starting.Median.Salary, main = "Histogram of Starting Median Salary", xlab = "Starting Median Salary")

qqnorm(log(fDTNoOut$Starting.Median.Salary), main = "log(Starting Median Salary) (Without Outliers) Q-Q Plot")
qqline(log(fDTNoOut$Starting.Median.Salary))

hist(log(fDTNoOut$Starting.Median.Salary), main = "Histogram of log(Starting Median Salary) (Without Outliers)", xlab = "log(Starting Median Salary)")

ols.full <- lm(Starting.Median.Salary ~ CostIState + CostOState + Undergraduates + AdmitRate + Applicants, data=fDTNoOut)

collin.diag = colldiag(mod=ols.full, scale=TRUE, center=TRUE, add.intercept=TRUE)

collin.diag

ols.full <- lm(Starting.Median.Salary ~ CostOState + Undergraduates + CollegeType +
EntranceDifficulty + AdmitRate + Applicants + Region, data=fDTNoOut)

vif(ols.full)

#OLS assumptions:
 #YC (Y is continuous): No
 #EN (Errors are normally distributed): Yes
 #XI (X's are independent): Yes
 #LI (Y and X's have linear reltionship): Yes
 #OI (Observations are independent): Yes
 #EI (Errors are independent): Yes
 #EA (The error average is 0): 
 #EV (The error variance is constant (homoscedasticity)): Yes
```

```{r training split}
set.seed(123)
train <- sample(1:nrow(fDTNoOut), 0.8*nrow(fDTNoOut))
```


```{r OLS Test - Y is Continuous}

#Y cannot be less then 0, as tuition, salary, and acceptance rate cannot be below 0, therefore Y is not continuous.

```


```{r OLS Test - Errors are Normally Distributed}
fdt.full=lm(Starting.Median.Salary~., data=fullDataTraining)

plot(fdt.full)
```

```{r OLS Assumptions - Xs are Independent}
# collin.diag = colldiag(mod=fdt.full, scale=FALSE, center=FALSE, add.intercept=TRUE)
# collin.diag
```

```{r OLS Assumptions - Error Variance is Constant}
lm.formula=Starting.Median.Salary~.
bptest(lm.formula, data=fullDataTraining)
```

BP-Test resulted in p-value = 0.3638, so unlikely Heteroscedasticity is present.


```{r OLS Assumptions - Observations/Errors are Independent}
#Durbin-Watson Test
#dwtest(fdt.full) 
```


##Section 5 Model Justification (20 pts.) 

A justification for the analytics methods and model specifications used. All methods used must be appropriate and relevant to the problem and you need to provide a justification for the selected methods based on: 

a.	Conformance with or departure from OLS assumptions; If you are using OLS as one of your methods, you must test key OLS assumptions. If you don’t plan to use the OLS model, you need to specify which OLS assumption would be violated. 

As shown in the prior section, our dataset conforms with these OLS assumptions: errors are normally distributed, X's are independent,	Y and X's have linear reltionship, observations are independent, errors are independent, the error average is 0, and error variance is constant. The assumptions which our dataset departs from OLS is Y is continuous. We find that because the errors are normally distributed and Y is now normally distributed it is still ok to run OLS even though Y is not continuous.

b.	Analysis goals (i.e., inference, interpretation and/or accuracy); you must specify your analysis goal(s). 

If university can lead to a better paying job down the road does the university matter. The goal of the project is to show how a choice in a school can lead to this difference. Therefore our goal is inference and interpretablity of the result.This allows for the results to be useful for both students and student's families and schools. 

c.	Predictive accuracy based on cross-validation test statistics. Similarly, the particular model specifications utilized must have a rationale. For example, if you chose a quadratic regression specification, you must have some rationale for the respective non-linear relationship. All projects must be analyzed with a variety of appropriate model with different model specification. Please consult with me if in doubt, but these are the minimum requirements 

The outcome variable our project is trying to explain is the Starting Median Salary. The predictors used were Cost of Out of State Tuition, the Number of Undergraduates, Private/Public School, Percieved Entrance Difficulty, Admission Rate, the Number of Applicants and the Region.

The models used were OLS, WLS, and Piecewise because of their ability to be used for interpretability and inference. The first specification used was No Transformation which incorporated the all the variables listed above without transformations, to establish a baseline of the models. The next specification was The Log transformed outcome variable, but all predictors remained the same. This was used because y showed signs of not being continuous. Lastly an interaction effect between number of undergraduates and college type because public schools typically have a higher population of students then privates schools so this difference was thought to be caught with an interaction variable between the two.

The best model was a plain Weighted Least Squares Regression where the outcome variable was not logged and there was no interaction effect. We chose this not only because it has the lowest test MSE, but also because it weighted certain variables to correct for errors that would have been seen in the ols model. This variable Undergraduates was seen as not significant in the model, but we though it was relevant because of it is possible that it is also a proxy for class sizes which could effect the quality of a graduates education i.e. larger schools potentially have larger class sizes.


<!-- Check Figure Size and If All Packages are Needed -->

All projects must evaluate 3 different methods, with 2 different model specifications for each. You need to provide in this section a sound justification for your choice of models to evaluate. 

<!-- Used echo = FALSE and eval = FALSE to prevent the code chunks for the piecewise AdmitRate from being ran --> 

```{r OLS No Transformation}
ols.noTransformation <- lm(Starting.Median.Salary ~ CostOState + Undergraduates + CollegeType + EntranceDifficulty + AdmitRate + Applicants + Region, data=fDTNoOut[train,])
ols.noTransformation.mse.test <- mean((fDTNoOut$Starting.Median.Salary-predict(ols.noTransformation,fDTNoOut))[-train]^2) 
```

```{r OLS Logged}
ols.logged <- lm(log(Starting.Median.Salary) ~ CostOState + Undergraduates + CollegeType +EntranceDifficulty + AdmitRate + Applicants + Region, data=fDTNoOut[train,])
ols.logged.mse.test <- mean((fDTNoOut$Starting.Median.Salary-exp(predict(ols.logged,fDTNoOut)))[-train]^2) 
```

```{r OLS Interaction}
ols.interaction <- lm(Starting.Median.Salary ~ CostOState + EntranceDifficulty + 
                 CollegeType*Undergraduates + Applicants + AdmitRate + Region, data=fDTNoOut[train,])
ols.interaction.mse.test <-
mean((fDTNoOut$Starting.Median.Salary-predict(ols.interaction,fDTNoOut))[-train]^2) 
```

```{r WLS No Transformation}
wls.noTransformation <- lm(Starting.Median.Salary ~ CostOState + Undergraduates + CollegeType +
EntranceDifficulty + AdmitRate + Applicants + Region, data=fDTNoOut[train,])

lm.formula <- Starting.Median.Salary ~ CostOState + Undergraduates + CollegeType +
EntranceDifficulty + AdmitRate + Applicants + Region

lm.fit.wls1=lm(lm.formula,data=fDTNoOut[train,], weights=1/wls.noTransformation$residuals^2)

wls.noTransformation.mse.test <- mean((fDTNoOut$Starting.Median.Salary-predict(lm.fit.wls1,fDTNoOut))[-train]^2)
```

```{r WLS Logged}
wls.logged <- lm(log(Starting.Median.Salary) ~ CostOState + Undergraduates + CollegeType +
EntranceDifficulty + AdmitRate + Applicants + Region, data=fDTNoOut[train,])

lm.formula <- log(Starting.Median.Salary) ~ CostOState + Undergraduates + CollegeType +
EntranceDifficulty + AdmitRate + Applicants + Region

lm.fit.wls2=lm(lm.formula,data=fDTNoOut[train,], weights=1/wls.logged$residuals^2)

wls.logged.mse.test <- mean((fDTNoOut$Starting.Median.Salary-exp(predict(lm.fit.wls2,fDTNoOut)))[-train]^2)
```

```{r WLS Interaction}
wls.interaction <- lm(Starting.Median.Salary ~ CostOState + EntranceDifficulty + 
                 CollegeType*Undergraduates + Applicants + AdmitRate + Region, data=fDTNoOut[train,])

lm.formula <- Starting.Median.Salary ~ CostOState + EntranceDifficulty + 
                 CollegeType*Undergraduates + Applicants + AdmitRate + Region
lm.fit.wls3=lm(lm.formula,data=fDTNoOut[train,], weights=1/wls.interaction$residuals^2)
wls.interaction.mse.test <- mean((fDTNoOut$Starting.Median.Salary-predict(lm.fit.wls3,fDTNoOut))[-train]^2)
```

```{r Piecewise Setup}
admitlimits=range(fDTNoOut$AdmitRate)
admit.seq=seq(from=admitlimits[1],to=admitlimits[2])
plot(fDTNoOut$AdmitRate,fDTNoOut$Starting.Median.Salary,xlim=admitlimits,cex=.5,col="black", xlab = "Admit Rate", ylab = "Starting Median Salary", main = "Admit Rate vs. Starting Median Salary")
# Knots look to be at AdmitRate .155 and AdmitRate .6
abline(v=c(.155,.6), col="red", lty=2, lwd=1)
```

```{r Piecewise AdmitRate, echo=FALSE, eval=FALSE}
fit.piecewise=lm(Starting.Median.Salary~AdmitRate+I((AdmitRate-.155)*(AdmitRate>.155))+I((AdmitRate-.6)*(AdmitRate>.6)), data=fDTNoOut)
summary(fit.piecewise)
```

```{r Piecewise No Transformation}
piecewise.noTransformation=lm(Starting.Median.Salary~ CostOState + EntranceDifficulty + 
                 CollegeType+Undergraduates + Applicants + Region+
                   AdmitRate+I((AdmitRate-.155)*(AdmitRate>.155))+I((AdmitRate-.6)*(AdmitRate>.6)), data=fDTNoOut[train,])
pw.noTransformation.mse.test <- mean((fDTNoOut$Starting.Median.Salary-predict(piecewise.noTransformation,fDTNoOut))[-train]^2)
```

```{r Piecewise Logged}
piecewise.logged=lm(log(Starting.Median.Salary)~ CostOState + EntranceDifficulty + 
                 CollegeType+Undergraduates + Applicants + Region+
                   AdmitRate+I((AdmitRate-.155)*(AdmitRate>.155))+I((AdmitRate-.6)*(AdmitRate>.6)), data=fDTNoOut[train,])
pw.logged.mse.test <- mean((fDTNoOut$Starting.Median.Salary-exp(predict(piecewise.logged,fDTNoOut)))[-train]^2)
```

```{r Piecewise Interaction}
piecewise.interaction=lm(Starting.Median.Salary~ CostOState + EntranceDifficulty + 
                 CollegeType*Undergraduates + Applicants + Region+
                   AdmitRate+I((AdmitRate-.155)*(AdmitRate>.155))+I((AdmitRate-.6)*(AdmitRate>.6)), data=fDTNoOut[train,])
pw.interaction.mse.test <- mean((fDTNoOut$Starting.Median.Salary-predict(piecewise.interaction,fDTNoOut))[-train]^2)
```

```{r MSE}
mse.table<- matrix(c(ols.noTransformation.mse.test, ols.logged.mse.test, ols.interaction.mse.test,
                     wls.noTransformation.mse.test, wls.logged.mse.test, wls.interaction.mse.test, 
                     pw.noTransformation.mse.test, pw.logged.mse.test, pw.interaction.mse.test),
                   ncol=3, byrow=TRUE)
colnames(mse.table) <- c("No Transformations","Logged", "Interaction")
rownames(mse.table) <- c("OLS","WLS","Piecewise")
mse.table <- as.table(mse.table)
mse.table
```

```{r Prediction}
summary(lm.fit.wls1)
```







##Section 6 Analysis of Results	 (20 pts.) 

Analysis and presentation of results. Your analysis and results need to contain some narrative to allow your audience to understand what you did. A simple output and diagram dump with no explanation will receive very little credit. Every procedure, output and diagram needs to be briefly but appropriately introduced before and briefly commented on its meaning after. Don’t leave it up to the reader to interpret what you did. 

##Section 7 Conclusion	(15 pts.) 

A short section with final thoughts, conclusions and lessons learned. These conclusions must contain a discussion of: 

a.	Your final model method and specification selection. Why did you pick this particular modeling method and model specification. 

b.	The main conclusions of your analysis. These conclusions must answer/solve your analytics question/problem stated in 1 above. Please be brief but concise and discuss the main insights you obtained from your analysis 

c.	A brief statement of your lessons learned in this project in terms of: data issues, methodological challenges, do's and don'ts, what you learned from this experience. You don't need to address all of this. But please be thoughtful and make it interesting. 

